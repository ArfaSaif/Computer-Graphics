
- ![image](https://user-images.githubusercontent.com/48233453/162028776-4387d011-5df9-4134-83bf-e4b42add8aa3.png)

- models that operate on the sequences
- specifically actually they operate on sets
- so you'd have a set of words which you can characterize as tokens 
-  then the transformer take all of these in and do  attention quadratic operation which basically means that you have to calculate the pairwise inner product between each of these between each pair of the of these bubbles which becomes a very very large task very quickly
- many many interconnections 
- 500 tokens long you need 500 squared connections so
